---
layout: A model-based clustering of multiple time-series algorithm
title: A model-based clustering of multiple time-series algorithm
categories: Academic project
---

# A model-based clustering of multiple time-series algorithm

This project is an pratical implementation of the methods in this article by an author.

## Introduction

The purpose of this project is to group a set of time series into different clusters and, in doing so, estimate the statistical model describing the time series of each cluster. To conduct our estimations, we adopt a Bayesian framework and utilize Monte Carlo simulation methods and hidden Markov chains.

Let ($y_{i,t}$), with $t=1,...,T$, be a multiple time series among N other time series $i=1,...,N$. We assume that these series belong to K different clusters, and that all time series belonging to cluster $k$ are described by the same statistical model with a group-specific parameter, $\theta_k$. The membership to group $k$ for each time series $i$ is initially unknown and is estimated simultaneously with the parameters $\theta_k$. Additionally, we assume that the parameters $\theta_k$ are specific to each cluster.

We introduce the vector $S = (S_1,S_2,...,S_N)$, where $\forall i \in 1,..,N$, $S_i$ $\in 1,...,K$ indicates the group to which time series $i$ belongs, and the vector $\phi = (\eta_1, ..., \eta_K)$ where $\eta_k$ indicates the proportion of time series belonging to cluster $k$.

Using an MCMC algorithm, we will iteratively estimate the vector $S$, followed by the vectors $\theta$ and $\phi$.

## The Model

### Theoretical Perspective

For $i = 1,..,N$, the conditional density of $y_i$ given $\theta_{S_i}$ is written as:

$p(y_i | \theta_{S_i}) = \prod_{t=1,...,T} p(y_{i,t}|y_{i,t-1},..,y_{i,0},\theta_{S_i})$

Where $p(y_i|y_{i,t-1},..,y_{i,0},\theta_{S_i})$ is a known density that depends on the chosen model.

Therefore,

$p(y_i | S_i, \theta_1,...,\theta_K) = p(y_i | \theta_{S_i})$

$=p(y_i | \theta_{1})$ if $S_i = 1$
...
$=p(y_i | \theta_{K})$ if $S_i = K$

Next, we establish a probabilistic model for the variable $S = (S_1,..,S_N)$. We assume that $S_1, S_2,..,S_N$ are pairwise independent a priori, and for all $i = 1,..,N$, we define the prior probability $Pr(S_i = k)$, the probability that time series $i$ belongs to cluster $k$. We assume that for each series $i$, we have no prior knowledge of which cluster it belongs to. Hence,

$Pr(S_i = k | \eta_1,..,\eta_K) = \eta_k$

The sizes of the groups $(\eta_1,..,\eta_K)$ are initially unknown and are estimated using the data.

### The MCMC Algorithm

The estimation of the parameter vector $\psi = (\theta_1,..,\theta_k,\phi,S)$ using MCMC is done in two steps:

**Step 1**
We fix the parameters $(\theta_1,..,\theta_K,\phi)$ and estimate $S$.

In this step, we will assign a group $k$ to each time series $i$ using the posterior $p(S_i|y,\theta_1,..,\theta_K,\phi)$.

Based on Bayes' theorem and the above, we know that:
$p(S_i = k|y,\theta_1,..,\theta_K,\phi) \propto p(y_i|\theta_k)Pr(S_i = k|\phi)$

Using equations (1) and (3), we will calculate this posterior for $k = 1,..,K$, and using Python, we will simulate a draw of $S_i$ and assign it to a group $k$.

**Step 2**
We fix the classification $S$ and estimate the parameter vector $(\theta_1,..,\theta_K,\phi)$.

Conditioned on $S$, the variables $\theta$ and $\phi$ are independent. Since the parameter $\theta_k$ is specific to cluster $k$, we group all time series belonging to group $k$.

Thus, $\theta_k$ is estimated using the posterior (5) and a Metropolis-Hastings algorithm:

$p(\theta_k|y,S_1,..,S_N) = \prod_{i : S_i = k} p(\theta_k|y_i) = \prod_{i : S_i = k} p(y_i|\theta_k)p(\theta_k)$

Where the prior $p(\theta_k)$ depends on the chosen model.

Finally, we estimate $\phi = (\eta_1,..,\eta_k)$ using the posterior (6) and a Metropolis-Hastings algorithm:

$p(\phi|S,y) = p(y|S,\phi,\theta_1,..,\theta_K) = p(y|S,\phi,\theta_1,..,\theta_K) \times p(S|\phi) \times p(\phi)$

$= \prod_{k=1,...,K} \prod_{i : S_i = k} p(y_i|\theta_k) \prod_{j = 1,...,N}Pr(S_j|\phi)p(\phi)$

Where the prior distribution of $\phi$ is a Dirichlet distribution (4,..,4).

We will estimate $\psi = (\theta_1,..,\theta_k,\phi,S)$ by repeating these two steps P times, after initializing $\psi^{0} = (\theta_1^{0},..,\theta_k^{0},\phi^{0},S^{0})$.

## Implementation

From a practical point of view, the likelihood of the estimated ARIMAX(p, 0, d) models was calculated using the statsmodels library.

To avoid a final classification with zero time series in a cluster, we decided to randomly select about ten series in such cases to update the cluster parameters. Otherwise, if the size of a cluster is reduced to zero in the early iterations, the coefficients associated with the model would not be updated.

The two steps described in the previous section were used for a Gibbs sampling algorithm. For each step, a Metropolis-Hastings algorithm was implemented. A random walk was performed to find the model coefficients, with ten successive iterations for each step. We selected this number based on the results we obtained and taking into account the complexity of the final algorithm.
